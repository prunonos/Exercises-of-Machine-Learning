{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Session 7 - Kernel SVM\n",
    "\n",
    "## 1 Introduction\n",
    "\n",
    "Welcome to the 7th exercise session of CS233 - Introduction to Machine Learning.  \n",
    "\n",
    "We will continue using scikit-learn to train SVM with feature expansion and different kernel functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from plots import plot, plot_expand, plot_expand_poly, plot_mykernel\n",
    "from sklearn import svm\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Non-Linear Classification\n",
    "Recap from exercise 6, we have worked on linearly separable data in the given data space. Most of the time that's not the case. In this exercise, we will work on non-linearly separable data. Although the dataset is not linearly separable in the original data space, it can be linearly separable in the non-linear feature space which  is defined implicitly by the non-linear kernel functions. Thus the training data points are perfectly separated in the original data space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond the linear problem we discussed before, SVM can also solve non-linear classification problem by using kernel functions. We replace $\\mathbf{x}_i$ with $\\phi(\\mathbf{x}_i)$, and then $\\mathbf{x}_i^T\\mathbf{x}_j$ with $k(\\mathbf{x}_i,\\mathbf{x}_j)$. The **dual form** of this problem is given by:  \n",
    "\\begin{align}\n",
    "    \\underset{\\{\\alpha_i\\}}{\\operatorname{max}} \\ \\ \n",
    "    & \\sum_{i=1}^N \\alpha_i - \\frac 1 2 \\sum_{i=1}^N\\sum_{j=1}^N \\alpha_i\\alpha_jy_iy_jk(\\mathbf{x}_i,\\mathbf{x}_j)  \\\\   \n",
    "    \\operatorname{subject \\ to} & \\ \\ \\sum_{i=1}^N \\alpha_iy_i = 0 \\\\\n",
    "                 & \\ \\ 0 \\leq \\alpha_i \\leq C, \\forall i \\ \\ \n",
    "\\end{align}\n",
    "**Question**\n",
    "   * How can you write $\\tilde{\\mathbf{w}}$ using $\\alpha_i$s and function $\\phi$?\n",
    "   * How is $y(\\mathbf{x})$ represented using $\\alpha_i$s?\n",
    " \n",
    "**Answer**\n",
    "   * $\\tilde{\\mathbf{w}} = \\sum_{i=1}^N \\alpha_iy_i\\phi(x_i) $\n",
    "   * We plugging the $\\tilde{\\mathbf{w}}$ as, \n",
    "     \\begin{align}\n",
    "       \\hat{y}(\\mathbf{x}) &= \\tilde{\\mathbf{w}}^T\\phi(\\mathbf{x}) + w_0 \\\\\n",
    "                           &= \\sum_{i=1}^N \\alpha_iy_ik(\\mathbf{x_i},\\mathbf{x}) + w_0\n",
    "     \\end{align}\n",
    "   * The sum can be computed on the support vectors ($\\delta$) only, \n",
    "       \\begin{align}\n",
    "       \\hat{y}(\\mathbf{x}) & = \\sum_{i \\in \\delta} \\alpha_iy_ik(\\mathbf{x_i},\\mathbf{x}) + w_0\n",
    "     \\end{align}\n",
    "   \n",
    "Have a look at the SVM function [here.](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) The main parameters you should look for are:\n",
    "- Kernel Functions: Linear, Polynomial and RBF ($X$ is the data)\n",
    "    - Linear: `linear`. $\\langle X, X' \\rangle $.\n",
    "    - Polynomial: `poly`. $( \\gamma \\langle X, X' \\rangle + r)^d $. $d$ is specified by keyword `degree`, $r$ by `coef0`.\n",
    "    - RBF: `rbf`. $\\exp(\\gamma ||X - X'||^2)$. $\\gamma$ is specified by keyword `gamma`, must be greater than 0.\n",
    "- Penalty term: C \n",
    "- Gamma: for Polynomial and RBF kernel\n",
    "- Degree: for Polynomial kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "from helpers import get_circle_dataset\n",
    "\n",
    "X,Y = get_circle_dataset()\n",
    "plot(X,Y,None,dataOnly=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is not linearly separable in the original two-dimensional data space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Linear SVM on original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Is a linear SVM able to separate the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SVM with linear kernel where C=0.1\n",
    "# and you can also try some other Cs to see what will happen\n",
    "clf_linear = svm.SVC(kernel='linear', C=0.1)\n",
    "    \n",
    "clf_linear.fit(X, Y)\n",
    "plot(X, Y, clf_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** Linear SVM fails on this non-linearly separable dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Polynomial SVM on the dataset from Polynomial Feature Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for Polynomial SVM, we first do polynomial feature expension to map the original data to a higher dimension. Then use a SVM with linear kernel to separate the expanded dataset.  \n",
    "\n",
    "Fill in the function `expand_X()`. You should add a bias term, but **omit the interaction terms**. An example:\n",
    "\n",
    "For $D=2$, $\\text{degree_of_expansion}=2$ you have:\n",
    "$$\n",
    "\\mathbf{x}_i = \\begin{bmatrix}\\mathbf{x}_i^{(0)}& \\mathbf{x}_i^{(1)}\\end{bmatrix}\n",
    "$$\n",
    "After the polynomial feature expansion, you would like to have:\n",
    "$$ \n",
    "\\mathbf{\\phi}(\\mathbf{x}_i) = \\begin{bmatrix}\\mathbf{1} & \\mathbf{x}_i^{(0)} & \\mathbf{x}_i^{(1)} & (\\mathbf{x}_i^{(0)})^2 & (\\mathbf{x}_i^{(1)})^2 \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform degree-d polynomial feature expansion of input data X\n",
    "def expand_X(X, degree_of_expansion):\n",
    "    \"\"\"  Perform degree-d polynomial feature expansion of X, \n",
    "         with bias but omitting interaction terms\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): data, shape (N, D).\n",
    "        degree_of_expansion (int): The degree of the polynomial feature expansion.\n",
    "    \n",
    "    Returns:\n",
    "        expanded_X (np.array): Expanded data with shape (N, new_D), \n",
    "                               where new_D is D*degree_of_expansion+1\n",
    "    \n",
    "    \"\"\"\n",
    "    expanded_X = np.ones((X.shape[0],1))\n",
    "    ### CODE HERE ###\n",
    "    for idx in range(1,degree_of_expansion+1): \n",
    "        expanded_X = np.hstack((expanded_X, X**idx)) \n",
    "        \n",
    "    return expanded_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial SVM\n",
    "degree_of_expansion = 2\n",
    "\n",
    "## Do polynomial feature expansion\n",
    "### CODE HERE ###\n",
    "expanded_X = expand_X(X, degree_of_expansion)\n",
    "\n",
    "print(\"The original data has {} features.\".format(X.shape[1]))\n",
    "print(\"After degree-{} polynomial feature expansion (with bias, without interaction terms) the data has {} features.\".format(degree_of_expansion,expanded_X.shape[1]))\n",
    "\n",
    "## Use SVM with linear kernel on expanded data with C=10.0\n",
    "### CODE HERE ###\n",
    "expanded_clf = svm.SVC(kernel='linear', C=10.0)\n",
    "expanded_clf.fit(expanded_X, Y)\n",
    "\n",
    "plot_expand(X, Y, expanded_clf, degree_of_expansion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the non-linearly separable dataset can be separated by the Polynomial SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4  SVM with Polynomial and RBF kernels\n",
    "\n",
    "We then try out Polynomial and RBF kernels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given data $\\mathbf{X}$ with $N$ samples, its kernel matrix $\\mathbf{K}$ is the $N \\times N$ symmetric Gram matrix with elelments \n",
    "\n",
    "$$ \\mathbf{K}_{n,m} = \\phi(\\mathbf{x}_n)^T\\phi(\\mathbf{x}_m) = k(\\mathbf{x}_n, \\mathbf{x}_m) $$\n",
    "\n",
    "Have a look at kernel functions defined in scikit-learn:\n",
    "\n",
    "- linear: $\\langle \\mathbf{X}, \\mathbf{X'} \\rangle $.\n",
    "- poly: $( \\gamma \\langle \\mathbf{X}, \\mathbf{X'} \\rangle + r)^d $. $d$ is specified by keyword `degree`, $r$ by `coef0`.\n",
    "- rbf: $\\exp(\\gamma ||\\mathbf{X} - \\mathbf{X'}||^2)$. $\\gamma$ is specified by keyword `gamma`, must be greater than 0.\n",
    "where $X$ is the data.\n",
    "\n",
    "Note that $\\phi$ **does not appear explicitly** in these kernel functions.\n",
    "\n",
    "Let's implement the polynomial kernel function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preform your own polynomial kernel function\n",
    "## Refer to the formula of poly kernel function above\n",
    "def my_poly_kernel(X, Y, degree=3, gamma=1.0, coef0=1.0):\n",
    "    \"\"\"  Perform the degree-d polynomial kernel function on X and Y \n",
    "    Args:\n",
    "        X (np.array): data, shape (N, D).\n",
    "        Y (np.array): data, shape (N, D).\n",
    "        degree (int): The degree of the polynomial kernel method.\n",
    "    Returns:\n",
    "        K (np.array): the kernel matrix from data matrices; that matrix should be an array of shape (N, N).    \n",
    "    \"\"\"\n",
    "    ### CODE HERE ###\n",
    "    K = (gamma * np.dot(X, Y.T) + coef0) ** degree   \n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question a**  What are the differences between polynomial feature expansion and polynomial kernel function? \n",
    "\n",
    "**Question b**  Is the SVM trained with linear kernel on polynomially expanded data same as the SVM trained with polynomial kernel function on original data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will check our implementation of polynomial kernel and to figure out the answer of Question b.\n",
    "\n",
    "We will make use of [sklearn.preprocessing.PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) to do the polynomial feature expansion with **the interaction terms**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These three SVM should be same.\n",
    "\n",
    "degree_of_expansion = 3\n",
    "\n",
    "# Baseline \n",
    "# Use SVM with poly kernel on original data with C=10.0, gamma=1.0, coef0=1.0\n",
    "### CODE HERE ###\n",
    "clf = svm.SVC(kernel='poly', C=10., degree=degree_of_expansion, gamma=1.0, coef0=1.0)\n",
    "clf.fit(X, Y)\n",
    "\n",
    "plot(X, Y, clf)\n",
    "\n",
    "# Check my_poly_kernel\n",
    "# Use SVM with my_poly_kernel on original adata with C=10.0\n",
    "### CODE HERE ###\n",
    "    # tip: kernel=my_poly_kernel\n",
    "kernel_clf = svm.SVC(kernel=my_poly_kernel, C=10.)\n",
    "kernel_clf.fit(X, Y)\n",
    "\n",
    "plot_mykernel(X, Y, kernel_clf)\n",
    "\n",
    "# Answer Question b\n",
    "# Use SVM with linear kernel on expanded data\n",
    "## Use PolynomialFeatures to generate expanded data with the interaction terms\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "### CODE HERE ###\n",
    "poly = PolynomialFeatures(degree_of_expansion)\n",
    "poly_expanded_X = poly.fit_transform(X)\n",
    "\n",
    "print(\"The original data has {} features.\".format(X.shape[1]))\n",
    "print(\"After degree-{} polynomial feature expansion (with bias, with interaction terms) the data has {} features.\".format(degree_of_expansion,poly_expanded_X.shape[1]))\n",
    "\n",
    "## Use SVM with linear kernel on expanded data with C=10.0\n",
    "### CODE HERE ###\n",
    "poly_expanded_clf = svm.SVC(kernel='linear', C=10.0)\n",
    "poly_expanded_clf.fit(poly_expanded_X, Y)\n",
    "\n",
    "plot_expand_poly(X, Y, poly_expanded_clf, degree_of_expansion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer a** Polynominal feature expansion is applied on each sample while polynominal kernel function is used to  precompute the dot product between each pair of samples from polynomially expanded data in the training set and save it as the kernel for testing. The data transfomation is not explicit in kernel functions.\n",
    "\n",
    "**Answer b** Yes, the 2 SVMs are same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with different settings for different kernel functions (poly and rbf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SVM with polynomial kernel of different degrees\n",
    "D = 2 ** np.linspace(0, 6, num=7)\n",
    "for d in D:\n",
    "    ## use poly kernel with C=10., gamma=1.0, coef0=1.0 and different degrees\n",
    "    ### CODE HERE ###\n",
    "    clf = svm.SVC(kernel='poly', C=10., degree=d, gamma=1.0, coef0=1.0)\n",
    "    \n",
    "    clf.fit(X, Y)\n",
    "    plot(X, Y, clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SVM with RBF kernel with differen gammas\n",
    "G = np.logspace(-2,2,num=5)\n",
    "for g in G:\n",
    "    ## use rbf kernel with C=0.1 and different gammas\n",
    "    ### CODE HERE ###\n",
    "    clf = svm.SVC(kernel='rbf', C=0.1, gamma=g)\n",
    "    \n",
    "    clf.fit(X, Y)\n",
    "    plot(X, Y, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!!!** It is important to choose the approperiate parameters in the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Look at the given dataset again, which will be the best non-linear kernel in your mind? If you choose polynomial kernel, which degree will be the best? If you choose RBF kernel, which gamma will be the best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Let's use Grid Search and Cross Validation techniques to find out the answer. We can apply K-Fold cross validation to different sets of parameters. We make use of the mean of K obtained accuracies to figure out the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the K-Fold cross validation for RBF kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold cross validation for searching parameters of RBF kernel.\n",
    "from helpers import do_cross_validation, fold_indices\n",
    "\n",
    "# seach in log space\n",
    "grid_search_c = np.logspace(-4, 10, num=15)\n",
    "grid_search_gamma = np.logspace(-9, 5, num=15)\n",
    "\n",
    "#save the accuracies for the combination of hyperparameters\n",
    "grid_val = np.zeros((len(grid_search_c), len(grid_search_gamma)))\n",
    "\n",
    "# Do 4 fold cross validation\n",
    "k_fold = 4\n",
    "k_fold_ind = fold_indices(X.shape[0], k_fold)\n",
    "\n",
    "for i, c in enumerate(grid_search_c):\n",
    "    for j, g in enumerate(grid_search_gamma):\n",
    "        print('Evaluating for C:{} gamma:{} ...'.format(c, g))\n",
    "        \n",
    "        ## call SVM with c,g as params.\n",
    "        ### CODE HERE ####\n",
    "        clf = svm.SVC(kernel='rbf', C=c, gamma=g)\n",
    "        \n",
    "        acc = np.zeros(k_fold)\n",
    "        ## do cross validation\n",
    "        for k in range(k_fold):\n",
    "            acc[k] = do_cross_validation(clf, k, k_fold_ind, X, Y)\n",
    "            \n",
    "        ## fill out the grid_val by computing the mean accuracy from k_fold runs.\n",
    "        ### CODE HERE ####\n",
    "        grid_val[i,j] = np.mean(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots import plot_cv_result_rbf\n",
    "## show all results and the best one\n",
    "plot_cv_result_rbf(grid_val,grid_search_c,grid_search_gamma)\n",
    "print('Best acc:{}'.format(np.max(grid_val)))\n",
    "## best params\n",
    "cin,gin = np.unravel_index(np.argmax(grid_val),grid_val.shape)\n",
    "print('Best Params- C:{}, Gamma:{}'.format(grid_search_c[cin],grid_search_gamma[gin]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above heatmap shows accuracies for different Gamma and C values. The best params are used on test set.   \n",
    "**Question** Is there a relation between C and Gamma?   \n",
    "**Hint**: Think how increase in one value changes other. Look at the heatmap to get the idea. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: High Gamma will lead to overfitting, hence smaller C would more misclassification to counteract. Lower Gamma is underfitting and hence high C is to stop misclassification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we do the K-Fold cross validation for polynominal kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold cross validation for searching parameters of Polynominal kernel.\n",
    "from helpers import do_cross_validation, fold_indices\n",
    "\n",
    "grid_search_c = np.logspace(-5,5,num=11)\n",
    "grid_search_degree = 2 ** np.linspace(0, 8, num=9)\n",
    "\n",
    "#save the accuracies for the combination of hyperparameters\n",
    "grid_val = np.zeros((len(grid_search_c),len(grid_search_degree)))\n",
    "\n",
    "# Do 4 fold cross validation\n",
    "k_fold = 4\n",
    "k_fold_ind = fold_indices(X.shape[0],k_fold)\n",
    "\n",
    "for i, c in enumerate(grid_search_c):\n",
    "    for j, d in enumerate(grid_search_degree):\n",
    "        print('Evaluating for C:{} degree:{} ...'.format(c, d))\n",
    "        \n",
    "        ## call SVM with c,d as params.\n",
    "        ### CODE HERE ####\n",
    "        clf = svm.SVC(kernel='poly', C=c, degree=d, gamma=1.0, coef0=1.0)\n",
    "        \n",
    "        acc = np.zeros(k_fold)\n",
    "        # do cross validation\n",
    "        for k in range(k_fold):\n",
    "            acc[k] = do_cross_validation(clf,k,k_fold_ind,X,Y)\n",
    "            \n",
    "        ## fill out the grid_val by computing the mean accuracy from k_fold runs. \n",
    "        ### CODE HERE ####\n",
    "        grid_val[i,j] = np.mean(acc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots import plot_cv_result_poly\n",
    "## show all results and the best one\n",
    "plot_cv_result_poly(grid_val, grid_search_c, grid_search_degree)\n",
    "print('Best acc:{}'.format(np.max(grid_val)))\n",
    "## best params\n",
    "cin,gin = np.unravel_index(np.argmax(grid_val),grid_val.shape)\n",
    "print('Best Params- C:{}, Degree:{}'.format(grid_search_c[cin],grid_search_degree[gin]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
